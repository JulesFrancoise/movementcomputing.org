@inproceedings{10.1145/3537972.3537976,
author = {Cavdir, Doga and Dahl, Sofia},
title = {Performers’ Use of Space and Body in Movement Interaction with A Movement-Based Digital Musical Instrument},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537976},
doi = {10.1145/3537972.3537976},
abstract = {Movement-based musical interfaces support performers’ music and movement expressions by drawing from expertise and creative practices of both disciplines. In this work, we qualitatively and quantitatively analyze the movement interaction of participants with Bodyharp, a movement-based musical instrument. This wearable instrument offers musical affordances that allow performers to extend beyond small gestural spaces. Its wearable design encourages the performers to move while creating music and to express while using their bodies. Data was collected from twenty participants’ interactions, reflections, and compositions with Bodyharp. Video recordings of the experiment were annotated and qualitatively analyzed to reveal which performed gestures directly contribute to sound production and modification and which gestures accompany these musical actions. For a subset of participants, Musical Gestures Toolbox was used to further quantify the gestures. Using the Laban Movement Analysis framework, we observed participants’ use of space and body in their interaction with a movement-based musical instrument and how their backgrounds in music or movement (based on participants’ self-reported experiences) influenced the interaction. Our results offer design practices for creating new interactions at the intersection of music and dance.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {1},
numpages = {12},
keywords = {digital musical instrument design, movement-based interaction, dance practice, embodied interaction, human-centered design},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537977,
author = {Fulton, Stephanie and Aichele, Dexter and Coulon, Eva and Kizer, Mattew and Conser, Anna and Bares, William},
title = {Stage Together: Remote Rehearsal of Theater Blocking},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537977},
doi = {10.1145/3537972.3537977},
abstract = {The COVID-19 pandemic has forced many theater productions to shift to remote rehearsals and performances using online video conference platforms. Video conference software makes it possible to share individual performances, but loses the sense of presence together and provides no way to visualize blocking movements on the shared space of the stage. Rehearsal by video conference can cause productions to either omit blocking altogether or have only a few on stage rehearsals, sometimes just days before opening. This paper presents a Web-based system that enables productions to remotely rehearse blocking at any stage of the production. Remote performers can propose their own blocking, rehearse scripted cues and visualize movements in a shared virtual stage. This paper describes the design, implementation, and focus group evaluations of Stage Together with a college theater production of Medea by Euripides.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {2},
numpages = {5},
keywords = {film, motion capture, theater, augmented reality, remote collaboration, telematics},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537978,
author = {Bisig, Daniel},
title = {Generative Dance - a Taxonomy and Survey},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537978},
doi = {10.1145/3537972.3537978},
abstract = {Generative Art is a creative approach that has found applications in several artistic disciplines. In some of these disciplines, formalization has historically played an important role, which predisposes them for employing generative methods. In dance, the relationship to Generative Art is less obvious and the role of formalization is more contested than in other disciplines. This paper tries to contribute to an understanding of the specific role that Generative Art currently plays in dance. It does so by proposing a taxonomy of topics that cover both common and dance specific aspects of Generative Art. This taxonomy is used for comparing a wide diversity of generative works that have been created in the context of dance.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {3},
numpages = {10},
keywords = {generative art, dance and technology, taxonomy},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537979,
author = {Kim, Eugenia Sangmie and Haebich, Jayson and Cassinelli, Alvaro},
title = {Radiant Soma: Visualization of Movement Through Motion Capture and Lasers},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537979},
doi = {10.1145/3537972.3537979},
abstract = {Augmented Materiality Lab, School of Creative Media, City University of Hong Kong, acassine@cityu.edu.hkThe human body has historically been a constant source of fascination in the arts and sciences. With the advent of digital computing technology, the debate over the virtues and disadvantages of physical versus virtual bodies has increased exponentially. One of the greatest challenges is translating the “essence” of human movement into the digital realm from physical reality. Radiant Soma emphasizes the ephemerality of human movement by visualizing motion capture data with lasers within the context of Korean shamanism and Stone Tape theory. The installation of light and phosphorescence transforms inanimate objects into metaphorical shamans emitting a constant stream of lively spirits.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {4},
numpages = {5},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537980,
author = {Stergiou, Marina and Vosinakis, Spyros},
title = {Exploring Costume-Avatar Interaction in Digital Dance Experiences},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537980},
doi = {10.1145/3537972.3537980},
abstract = {Dance in the digital world is an area that has been explored vividly the last years with applications in education/learning, cultural heritage preservation, archiving and performance. Digital costume is a concept complimentary to dance that also evolves as technology grows, with virtual fashion shows, digital-only clothing, virtual showrooms, and recreation of historic costumes to make their appearance more and more often. However, their intersection hasn’t been quite explored yet although it has a lot to offer in areas like digital fashion design, animation, games and artistic performances. We envisage that combining realistically simulated clothes along with motion data can add a significant potential to movement research. Specifically for learning or exploring movement in digital environments, the representation of the dancer (3d avatar) is an important aspect to be examined, for example, specific digital clothes might enhance movement understanding and thus affect learning, in some cases. Having the hypothesis that different avatar and/or cloth visualizations might lead to different results in movement, an experiment with users has been conducted in an immersive Virtual Reality environment. We setup a testbed environment, where avatars of different forms (abstract, anthropomorphic, robot) and with different costumes could perform dances. The users were immersed in the environment and had to repeatedly follow the dance steps with all avatar-costume combinations. Their performance was then rated by experts regarding the general quality of movement, the rhythm, and the steps. The analysis of these results provided insights on the effect of costumes and avatar types on the above parameters and along with qualitative aspects, wishes to initiate a study around the limitations and opportunities of combining costume and dance in digital, interactive environments.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {5},
numpages = {6},
keywords = {Virtual Reality, HCI, Digital dance, Cloth simulation, Avatars, Digital clothes},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537981,
author = {Martin, Patrick and Sicchio, Kate and Dietzel, Charles and Olivo, Alicia},
title = {Towards A Framework For Dancing Beyond Demonstration},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537981},
doi = {10.1145/3537972.3537981},
abstract = {This paper presents a prototype framework for developing real-time human-robot performances and its application within a recent choreographic work for the stage. The framework allows dancers to teach new motions to robots, while also giving choreographers the ability to compose performances from pre-built and learned behaviors. To achieve this capability, we combine behavior-based robotics and learning from demonstration approaches to construct behaviors and compose them into a performance. Our learning algorithm, dancing-from-demonstration (DfD), allows dancers and choreographers to teach new phrases to the robot and specify choreographic motifs for the performance. This collaborative work culminated in a human-robot duet, where the robot incorporates a new, learned motion into its choreography within live performance. These capabilities create a baseline for choreographers and dancers to eventually compose and perform in more dynamic, reactive choreo-robotic performances.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {6},
numpages = {7},
keywords = {choreography, learning-from-demonstration, autonomous robotics},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537985,
author = {Rogel, Amit and Savery, Richard and Yang, Ning and Weinberg, Gil},
title = {RoboGroove: Creating Fluid Motion for Dancing Robotic Arms},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537985},
doi = {10.1145/3537972.3537985},
abstract = {Robotic motion has been studied for many purposes, such as effective fast movements, communicative gestures, and obstacle avoidance. Through this study, we are able to improve the perceived expressivity of a robot performing a task by generating trajectories. So far, robots have been very rigid in their movements, making them feel more robotic and less human. The concept of follow through, and smooth movement, can be used to increase animacy for a robot which leads to a more lifelike performance. In this paper, we describe the use of follow through to improve dances and grooves that can match the elegance of human dancers. We created two techniques using a non-humanoid robotic arm, to simulate this for a robotic dancer. Our first technique uses forward kinematics with trajectories that are generated based on a dancer moving to a beat. We mapped various movements of a human dancer to a set of joints on a robotic arm to generate the dancing trajectory. This technique allows a robot to dance in real-time with a human dancer, and also create its own smooth trajectory. The time delay that each human body part uses is implemented as time delay in the robot movements. The second method uses impedance control with varied damping parameters to create follow through. Robotic joints with low damping can passively move in response to other movements in a robot. The arm had high damping for any active moving joints, while the rest of the arm would passively react to this excitation; similar to how a human body responds to our own movement. These two methods were compared in a study where the robot would dance to a beat and users would qualitatively and quantitatively rate the robotic movement. The results of the survey showed that both methods provided an increase in animacy and anthropomorphism of a robot dancing to a beat. Impedance control had the highest rating for animacy, anthropomorphism, full body, and individual body rating. The use of impedance can provide a simple way for a robot to dance like a human, without a change to the primary trajectory.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {7},
numpages = {9},
keywords = {robot perception, fluid motion, robot dancing, groove, robotics, follow through, impedance control},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537983,
author = {Krzyzaniak, Michael and Bishop, Laura},
title = {Professor Plucky: Expressive Body Movement in Human–Robot Musical Ensembles},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537983},
doi = {10.1145/3537972.3537983},
abstract = {When people play music together, they move their bodies, and that movement plays an important role in the activity of group music making. In contrast, when robots play music with people, the robots are usually stiff and mechanical in their movement. In general, it is not well understood how the movement of such robots affects how people interact with them, or how the robot movement should be designed in order to promote certain features of interaction. As an initial exploration into these questions, we built a prototype guitar plucking robot that plucks the strings with either a) kinetic plucking mechanisms that are designed to have visually appealing movement, or b) control plucking mechanisms that do not visually move. In a pilot study we found that when guitarists play with the robot, they move their hands more and look at the robot more when it uses the kinetic mechanisms as opposed to the control ones. However, they do not report preferring the kinetic mechanisms. These preliminary findings suggest some very clear hypotheses for future followup studies.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {8},
numpages = {8},
keywords = {NIME, movement, interaction, mocap, robots, MOCO, gesture, plucky, professor plucky, guitar, expression, musical robots, eye-tracking, music},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537984,
author = {Otterbein, Robin and Jochum, Elizabeth and Overholt, Daniel and Bai, Shaoping and Dalsgaard, Alex},
title = {Dance and Movement-Led Research for Designing and Evaluating Wearable Human-Computer Interfaces},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537984},
doi = {10.1145/3537972.3537984},
abstract = {Movement research and practice in the context of wearable technologies and human-computer interaction (HCI) shifts the design paradigm to the lived body. Human movement is characterized by sense, intention and expressiveness. Designing HCI from this standpoint opens up new possibilities to make computational devices and applications more accessible and integrated. This work presents an iterative, collaborative, and cross-disciplinary approach using wearable sensor bands in an open-ended performative exploration in exchange with a professional dancer. The goal is to understand the benefits and challenges of using movement-centered tools originating from dance practice and movement research and how they might feed back into the design, development and evaluation process of embodied technologies to improve human-computer interactions. Movement analysis systems and motion computation models are reviewed and leveraged in an interactive audiovisual system, with focus on using force-sensing resistors for low-level motion descriptors and Laban Movement Analysis for higher-level movement features. The artistic methodology, which combines practice and research, results, discussion of the iterative and collaborative process, and the final system architecture are the main topics presented in the paper.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {9},
numpages = {9},
keywords = {Interactive Dance, Interactive Systems, Audiovisual Feedback, Human-Computer-Interaction, Multimodal Wearable Sensors},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537986,
author = {Bench, Harmony and Elswit, Kate},
title = {“So… Will You Be Looking at Dance?”: Data-Led Dance History and the Edges of Movement Computing},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537986},
doi = {10.1145/3537972.3537986},
abstract = {Dance historical analysis demands an understanding of “movement data” that includes not just data seen to represent moving bodies in terms of dancing bodies, but also representing movement itself as a way of thinking and knowing, of archiving and transmission. In this paper, we draw on experiences from running Dunham's Data: Katherine Dunham and Digital Methods for Dance History Inquiry to argue that dance history can participate in initiatives to build more expansive understandings of measurement, analysis, and representation in movement computing, in particular by addressing kinds of movement that, while not “dancing” per se, are the enabling conditions for dance. We begin with contexts for a data-led dance history, and then elaborate the methods in terms of building, analyzing, and visualizing datasets that focus on transnational, intercorporeal, and intergenerational transmission of dance-based knowledge practices. We collect key findings regarding the capacity of such methodologies to expand the scope of historical movement inquiry, and in turn, to rethink the complexity of embodiment in a broader range of digital analytical contexts. Finally, the conclusion touches on further research, including the sensory potential of visualizing historical dance data in and as movement.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {10},
numpages = {8},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537987,
author = {Cuykendall, Shannon and Schiphorst, Thecla},
title = {The I-TEC Design Framework for Kinesthetic Transmission in Online Spaces},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537987},
doi = {10.1145/3537972.3537987},
abstract = {Disseminating dance in online spaces provides an opportunity for kinesthetic knowledge to reach broader audiences. However, the transmission of dance in online spaces also primarily relies on visual and aural modalities that cannot fully capture the nuanced physical sensations of a kinesthetic experience. In recent years there has been an influx of interactive online dance resources; yet there is little analysis of how these works effectively translate kinesthetic knowledge to online audiences. We bring together research in dance film and interaction design practices to explore kinesthetic transmission in online spaces and conduct analyses on interactive digital dance resources. Based on our literature review and analyses of these dance resources we propose the I-TEC design framework for kinesthetic transmission. In this framework, Instructional, Translational, Exploratory, and Contextual interactions are brought together to expose the multiple embodiments, perspectives, and translations of kinesthesia.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {11},
numpages = {12},
keywords = {digital dance documentation, screendance, dance film, kinesthetic interaction design, kinesthetic awareness, kinesthetic empathy},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537988,
author = {Rajko, Jessica},
title = {Geocultural Precarities in Canonizing Computing Research Involving Dance: The Effects of Treating Western, EuroAmerican Concert Dance as Universally Applicable across Geoculturally Diverse Movement Computing Research Efforts},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537988},
doi = {10.1145/3537972.3537988},
abstract = {This paper conducts a thorough mapping study and rhetorical analysis of computing research involving dance. Research investigates: 1) who conducts computing research involving dance; 2) how dance is described in computing research publications; and 3) how geoculturally-specific forms of embodied dance knowledge become normalised over time. The study's publication corpus is extracted from the Association of Computing Machinery (ACM) Digital Library and includes 135 papers returned in a database query using the general keyword search term “dancer.” Results illustrate the geocultural specificity of computing research involving dance and identify rhetorical trends that treat embodied knowledge and methods deriving from western concert dance as the unofficial norm of dance-based movement expertise in computing contexts. The paper's summaries and discussion consider the effects treating embodied knowledge deriving from western concert dance as universally applicable across geoculturally diverse movement analysis efforts. Here, I particularly address the unintended erasure of embodied knowledge deriving from non-western dance forms and discuss what we might learn in terms of best practices from existing trends in computing research involving non-western dance forms.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {12},
numpages = {14},
keywords = {non-western dance forms, geocultural norms, literature review, computational movement analysis, western concert dance},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537989,
author = {Mainsbridge, Mary},
title = {Feeling Movement in Live Electronic Music: An Embodied Autoethnography},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537989},
doi = {10.1145/3537972.3537989},
abstract = {Movement-based interaction relies on the measurement and abstraction of human motion. Yet reducing physical experience to quantitative and visual representations overlooks the inner perceptions and sensations that accompany movement. Rather than treating the body as an object to be observed and analysed, soma-based design approaches instead reflect first-person perspectives of felt experience, providing insight into how the body experiences and interacts with the world. Applying these methods to the musical realm, this paper offers a personal account of working with motion-sensing interfaces, exploring the underlying qualities and meanings of performance actions in live and recorded contexts. The embodied authethnographic study serves as an opportunity to question normative approaches to technology that favour techno-scientific narratives of external control and surveillance. Alternative values of agency, autonomy, empathy and transparency are investigated using first-person methodologies.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {13},
numpages = {7},
keywords = {Embodied musical interaction, Autoethnography, Kinaesthetic awareness, Gestural system, Soma-based design},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537990,
author = {Oyallon-Koloski, Jenny and Junokas, Michael},
title = {Enhancing Film Choreography Through Digital Representation of Camera Movement and Agency},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537990},
doi = {10.1145/3537972.3537990},
abstract = {Cinematography—aspect ratios, framing, and camera movement, especially—plays an essential role in film choreography. However, the camera’s capacity to expand the aesthetics of dance on film and contribute to the dynamism of figure movement is limited. In commercial (profit-oriented) cinematic practice, this limitation is bounded by the physical properties of the equipment, accessibility issues, limited time and financing for stylistic experimentation, and institutional memory loss of cinematic choreography techniques. The result in much of contemporary commercial dance on film is an emphasis on multi-camera coverage, tighter framings, and the use of editing to guide the dynamism of the figure movement in lieu of an emphasis on the relationship between the camera and the body. In this paper, we present theoretical frameworks and motion-capture driven utilities that empower the filmic choreographer beyond traditional, physical limits of the medium. We do so by providing digital representations and interactions using abstracted, artificial systems mimicking the live camera-dancer relationship that prioritize cinematic agency and movement as the principal subject material. This development parallels the growth of previsualization and camera motion control in the field of visual effects, linking us conceptually to existing industrial paradigms. Our initial foray into the expansion of this agency defines a progressive development of filmic choreography, escaping narrow limits of the traditional medium and provoking a more inclusive, accessible, and empowered form of creation through novel access of conventionally unmeasurable capability.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {14},
numpages = {5},
keywords = {cinema, dance, agency, cinematography, motion-capture, Laban/Bartenieff Movement Studies, motion control},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537991,
author = {Tsuchida, Shuhei and Mao, Haomin and Okamoto, Hideaki and Suzuki, Yuma and Kanada, Rintaro and Hori, Takayuki and Terada, Tsutomu and Tsukamoto, Masahiko},
title = {Dance Practice System That Shows What You Would Look Like If You Could Master the Dance},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537991},
doi = {10.1145/3537972.3537991},
abstract = {This study proposes a dance practice system allowing users to learn dancing by watching videos in which they have mastered the movements of a professional dancer. Video self-modeling, which encourages learners to improve their behavior by watching videos of exemplary behavior by themselves, effectively teaches movement skills. However, creating an ideal dance movement video is time-consuming and tedious for learners. To solve this problem, we utilize a video generation technique based on deepfake to automatically generate a video of the learners dancing the same movement as the dancer in the reference video. We conducted a user study with 20 participants to verify whether the deepfake video effectively teaches dance movements. The results showed no significant difference between the groups learning with the original and deepfake videos. In addition, the group using the deepfake video had significantly lower self-efficacy. Based on these experimental results, we discussed the design implications of the system using the deepfake video to support learning dance movements.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {15},
numpages = {8},
keywords = {video self-modeling, learning, dance movements, skill acquisition, deepfake},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537998,
author = {Fran\c{c}oise, Jules and Meseguer-Brocal, Gabriel and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Movement Analysis and Decomposition with the Continuous Wavelet Transform},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537998},
doi = {10.1145/3537972.3537998},
abstract = {Human movements support communication, and can be used to imitate actions or physical phenomenons. Observing gestural imitations of short sounds, we found that such gestures can be categorized by their frequency content. To analyse such movements, we propose an analysis method based on wavelet analysis for clustering or recognizing movement characteristics. Our technique draws upon the continuous wavelet transform to derive a time-frequency representation of movement information. We propose several global descriptors based on statistical descriptors, frequency tracking, or non-negative matrix factorization, that can be used for recognition or clustering to highlight relevant movement qualities. Additionally, we propose a real-time implementation of the continuous wavelet transform based on a set of approximations, that enables its use in interactive applications. Our method is evaluated on a database of gestures co-executed with vocal imitations of recorded sounds.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {16},
numpages = {13},
keywords = {Wavelet, Movement, Rhythmic Gesture, Vocalization, Recognition, Continuous Wavelet Transform},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537993,
author = {Knight, Jamal and Johnston, Andrew and Berry, Adam},
title = {Machine Art: Exploring Abstract Human Animation Through Machine Learning Methods},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537993},
doi = {10.1145/3537972.3537993},
abstract = {Visual media and performance art have a symbiotic relationship. They support one another and engage the audience by providing an experience or telling a story. This comparative study explores the accuracy, efficiency, and cost factors of using machine learning based motion capture methods in performance art. There is extensive research in the field of machine learning methods for human pose estimation, but the outputs of such work are rarely used as inputs for performance art. In this paper we present a practice-based research project that involves producing animations that match a performer's movements using machine learning based motion capture methods. We use human poses derived from low-cost video capture as an input into high-resolution abstract forms that accompany and synchronise with dance performances. A single-camera approach is examined and compared to existing methods. We find that compared with existing motion capture methods the machine learning based methods require less setup time, and less equipment is required resulting in considerably lower cost. This research suggests that machine learning has considerable potential to improve the quality of human pose estimation in performance art, visual effects and motion capture, and make it more accessible for arts companies with limited resources.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {17},
numpages = {7},
keywords = {keypoint detection, performance art, animation, motion capture, Practice-based, machine learning},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537999,
author = {Chassat, Perrine and Park, Juhyun and Brunel, Nicolas},
title = {Analysis of Variability in Sign Language Hand Trajectories: Development of Generative Model},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537999},
doi = {10.1145/3537972.3537999},
abstract = {The analysis of human movement poses a well-known challenge that has already been addressed in various ways and needs to be adapted to the type of movement being considered. The focus here is on the analysis of hand movement in sign language. This study aims to characterize and model the different variations present in the data to develop a realistic generative model of hand movement in sign language. We identify two types of variations that play a key role in characterizing human movement: temporal variations and shape variations, i.e., variations in the speed of movement and the geometry of movement. However, separating these variations or understanding their relationship is a non-trivial task. A well-known model for the relationship between time, speed, and geometry is the 2/3 power-law demonstrated for several human movements, mainly constrained and planar. We find that the generalization of this law to a three-dimensional motion is not sufficient to explain variations in hand movement in sign language. We develop a new statistical modeling framework that is flexible and can respect the geometry of the movement signals. The two different variations are identified using the Frenet-Serret representation and modeled by mean geometry, mean speed, and their nonlinear transformations. The nonlinear variations in time and geometry are analyzed by functional principal component analysis. Then the generative model for the hand movement in sign language is built by imposing a joint probability model on the principal coefficients of these components.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {18},
numpages = {8},
keywords = {3D trajectories, space variability, hand movement, sign language, time variability, functional data analysis, generative model},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537995,
author = {Napier, Emily and Gray, Gavin and Oore, Sageev},
title = {Spectral Analysis for Dance Movement Query and Interpolation},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537995},
doi = {10.1145/3537972.3537995},
abstract = {While it is possible to quantify motion using various transforms or computational techniques, these representations may not be easy to interpret or reconstruct. In this paper, we focus on the problem of visualizing, querying, and manipulating dance components in the form of spectral features. Our first contribution is measuring the similarity of movements in a way that is robust to phase differences while identifying motions with similar pose frequencies. Our second contribution uses the similarity of pose spectra as a metric to drive the interpolation of a motion sequence towards target statistics. We identify the visual impact of these metrics on the characteristics of motion with input from experts in the dance field. These techniques are implemented to explore representations of dance that have the potential to be the basis of more intuitive choreography generation and educational tools for dance artists.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {19},
numpages = {7},
keywords = {machine learning, motion capture, dance},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537996,
author = {Trabelsi, Imen and Francoise, Jules and Bellik, Yacine},
title = {Sensor-Based Activity Recognition Using Deep Learning: A Comparative Study},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537996},
doi = {10.1145/3537972.3537996},
abstract = {With the wide availability of inertial sensors in smartphones and connected objects, interest in sensor-based activity recognition has risen. Yet, recognizing human actions from inertial data remains a challenging task because of the complexity of human movements and of inter-individual differences in movement execution. Recently, approaches based on deep neural networks have shown success on standardized activity recognition datasets, yet few works investigate systematically how these models generalize to other protocols for data collection. We present a study that evaluates the performance of various deep learning architectures for activity recognition from a single inertial measurement unit, on a recognition task combining data from six publicly available datasets. We found that the best performance on this combined dataset is obtained with an approach combining the continuous wavelet transform and 2D convolutional neural networks.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {20},
numpages = {8},
keywords = {Movement Computing, Continuous Wavelet Transform, Deep Learning, Inertial Sensors, Convolutional Neural Networks., Human Activity recognition},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3537997,
author = {Soga, Asako and Umino, Bin and Hirayama, Motoko},
title = {Experimental Creation of Contemporary Dance Works Using a Body-Part Motion Synthesis System},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537997},
doi = {10.1145/3537972.3537997},
abstract = {We developed a body-part motion synthesis system (BMSS) that synthesizes 3D motion data captured from performances of professional dancers to support the creation of contemporary dance works. To evaluate the usefulness of the system, three professional choreographers created their original dance works experimentally using the BMSS three times, and dancers performed their works in theaters. By analyzing the sequence data created by the BMSS obtained through an interview with the choreographers, we found that the choreographers could discover a variety of uses for the BMSS by becoming proficient in its use. The characteristics of the choreography created by each choreographer were also clarified.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {21},
numpages = {5},
keywords = {Motion synthesis, Performance, Dance, Creation, Choreography},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538000,
author = {Artacho, Adri\'{a}n and Horstmeyer, Leonhard},
title = {SmoothOperator : A Device for Characterizing Smoothness in Body Movement},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538000},
doi = {10.1145/3537972.3538000},
abstract = {In our research we faced the problem of characterizing smoothness in human movement. Even though ’smooth’ is a common way to describe and conceptualize motion in the performing arts as well as in informal speech, we realized the need for a tool to differentiate between various degrees and modes of smoothness. We propose that smoothness operates at different interlocking orders. These appear only in aggregation, intertwined with other qualities of body movement. Akin to how a spectrometer splits light into a spectrum of frequencies, we developed a method to measure the degree of smoothness in each order, as an epistemic tool for dance practitioners to investigate the quality of body movement from a fresh perspective. To this end we have implemented a device that provides dancers with aural, haptic and visual feedback in real time, taking into account the constraints of a dance practice session.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {22},
numpages = {6},
keywords = {movement analysis, smoothness, performance tool, contemporary dance},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538001,
author = {Tajadura-Jimenez, Ana and Ley-Flores, Judith and Valdiviezo, Omar and Singh, Aneesha and Sanchez-Martin, Milagrosa and Diaz Duran, Joaquin and M\'{a}rquez Segura, Elena},
title = {Exploring the Design Space for Body Transformation Wearables to Support Physical Activity through Sensitizing and Bodystorming},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538001},
doi = {10.1145/3537972.3538001},
abstract = {Negative or disturbed body perceptions are often interwoven with people's physical inactivity. While wearables can support body perception changes (body transformation), the design space of body transformation wearables supporting physical activity remains narrow. To expand this design space, we conducted an embodied co-design workshop with users. Using conceptual and tangible sensitizing tools, we explored/reflected on bodily sensations at three moments of movement execution (before/during/after). Conceptual tools were used to evoke/reflect/capture past lived experiences, while tangible tools were used as ideation probes for sensory bodystorming. Two design concepts emerged, reflecting diverging approaches to body transformation wearables: one focused on reminders and movement correction; the other on sensory augmentation and facilitation. We reflect on how each facilitates useful representations of body sensations during movement, and present methodological recommendations for designing technology for sensory augmentation in this area. Finally, we propose a preliminary prototype based on our design concepts and discuss future steps.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {23},
numpages = {9},
keywords = {Probes, Self-tracking, Interactive Systems Design, Body Awareness, Psychological Factors, Body-Perception, Physical Activity, Health, Wearables, Bodystorming, Sound, Embodied Design Methods, Body Movement, Self-Care Technology Design, Technology Probes, Haptics, Embodied Cognition},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538002,
author = {Kalampratsidou, Vilelmini and Diamantides, Pandelis and Stergiou, Marina and El Raheb, Katerina and Ioannidis, Yannis and Issari, Philia and Georgaca, Eugenie and Skali, Dora and Koliouli, Flora and Karydi, Evangelia and Giokas, Panos and Vassilakou, Virginia and Pappas, Yannis},
title = {From Capturing the Embodied Social Experience to Music Composition: Data as Mediation},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538002},
doi = {10.1145/3537972.3538002},
abstract = {This work presents the first steps of a journey from capturing the embodied social experience to creating contemporary sound art through digital means. In the framework of the Transition to 8 project, residents of the Greek City of Eleusis expressed their feelings and perspectives on the social issues of their community through organized sociodrama sessions. The participants used wearable technology, resembling that of smart-watches, that enables the recording of various bodily data, such as heart rate, temperature, and skin conductance (affiliated with emotional arousal). The data collected are organised and analysed by applying a mixed-methods approach, and shared with artists to be used as inspiration and source material for their artistic productions. In Transition to 8 project we aim to build a platform and methodology that uses data as a means to communicate local social issues and citizen perspectives to an international community of artists. As a proof of concept one sound composition has been created so far, as an artistic piece, to guide this process, while the development of the platform and further experimentation are still an ongoing endeavor.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {24},
numpages = {4},
keywords = {heart rate, skin conductance, biosignals, sociodrama, body temperature, music composition, algorithmic music, Social issues},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538003,
author = {Akbas, Saliha and Evren Yantac, Asim and Eskenazi, Terry and Kuscu, Kemal and Semsioglu, Sinem and Topal Sumer, Onur and Ozturk, Asli},
title = {Virtual Dance Mirror: A Functional Approach to Avatar Representation through Movement in Immersive VR},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538003},
doi = {10.1145/3537972.3538003},
abstract = {Immersive Virtual Reality (VR) technologies offer new possibilities for studying embodied interaction with different sets of constraints and affordances for action-taking while using one’s physical body. In this study, we designed and prototyped a VR dance experience, Virtual Dance Mirror, where a dancer’s bodily movements are reflected on a 3D avatar model using a motion-capture suit. We investigated the novel possibilities for avatar design based on the expression of movements available for dancers in VR environment. After a preliminary briefing session, we conducted a user-study with five dancers with semi-structured interviews. Our findings support HCI literature on virtual body design to facilitate collaboration and non-verbal communication between VR users.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {25},
numpages = {4},
keywords = {dance, avatar representation, virtual reality, Embodiment, movement},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538006,
author = {Bares, William and Aichele, Dexter and Coulon, Eva and Hager, Nick and Wang, Tina and Le, Julian and Elawadly, Abdelrahman},
title = {Motion-Sensor Programming for Accessible Interfaces},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538006},
doi = {10.1145/3537972.3538006},
abstract = {Motion-sensing devices that track the movement of human bodies, rigid body marker clusters, and eyes offer a diverse range of alternative input modalities that can be applied to create more accessible interfaces for diverse user populations. We present a set of curricular modules and open-source software suitable for use at the two- or four-year college level to teach accessible design featuring programming of three different types of motion-sensing input devices. The intended audience are software developers or students in computing-related fields. The curricular modules are open source and include guidelines for accessible design, readings, lab activities and project assignment prompts.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {26},
numpages = {5},
keywords = {education, eye tracking, motion capture, user interfaces, accessibility},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538008,
author = {Bacula, Alexandra and Knight, Heather},
title = {Incoherent Robot Groups: How Divergent Motions within a Robot Group Predict Functional and Social Interpretations},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538008},
doi = {10.1145/3537972.3538008},
abstract = {Research has found homogeneous robot groups can be intimidating, but few have studied the impact of intentionally incoherent robot group motion. This work explores incoherent group motion through an exploratory online user study, varying how robots move relative to a human figure entering the scene. Online participants (N=240 participants) rated twelve research conditions across various social and functional goals. Results showed coherent groups had the strongest communication signals, but incoherent motion can cue more complex communications. Coherent motion towards was threatening and blocking, and coherent motion away was avoidant and harmless. Coherent stillness was inviting. Subgroup size linearly affected communication strength.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {27},
numpages = {6},
keywords = {multi-robot systems, social robotics, expressive motion},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538007,
author = {Toverud Ruud, Markus and Hisdal Sandberg, Tale and Johan Vedde Tranvaag, Ulrik and Wallace, Benedikte and Mojtaba Karbasi, Seyed and Torresen, Jim},
title = {Reinforcement Learning Based Dance Movement Generation},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538007},
doi = {10.1145/3537972.3538007},
abstract = {Generating genuinely creative and novel artifacts with machine learning is still a challenge in the world of computational science. A creative machine learning agent can be beneficial for applications where novel solutions are desired and may also optimize search. Reinforcement Learnings’ (RL) interactive properties can make it an effective tool to investigate these possibilities in creative contexts. This paper shows how a Reinforcement learning-based technique, in combination with Principal Component Analysis (PCA), can be utilized for generating varying movements based on a goal picking policy. The proposed model is trained on a data set of motion capture recordings of dance improvisation. Our study shows that the trained RL agent can learn to pick sequences of dance poses that are coherent, have compound movement, and can resemble dance.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {28},
numpages = {5},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538009,
author = {Cuykendall, Shannon and DiPaola, Steve},
title = {Floating Departures: Developing Quarantine Dance Technique as an Artistic Practice Beyond the Pandemic},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538009},
doi = {10.1145/3537972.3538009},
abstract = {We describe our process of quarantine dance technique in making the dance film and meditation, Floating Departures. This work, created during lockdown in 2021, brings together dance movement, poetry, painterly styles, and sound to explore cyclical patterns and points of departure in movement and life. To create Floating Departures we used a broad range of technologies–from everyday objects to smartphones to AI art systems. We experiment with various techniques to record ourselves and bring our movement together in a shared digital space with post-production video editing techniques. Using a bricolage approach, we incorporate materials, such as bubble wrap and balloons, to transform our spaces and explore our personal experiences during lockdown. We construct multiple layers of reality that are further transformed in unanticipated directions with AI technologies. Through our creation process, we develop a collective physical body to explore a new realm, unbound by reason or logic, that was only made possible through our remote collaborative processes and technologically-mediated interactions.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {29},
numpages = {7},
keywords = {Dance Film, Choreography, Bricolage, AI Art Systems, Screendance, Remote Creation},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538010,
author = {Brown, Courtney Douglass and Greenberg, Ira and Brimhall, Brent},
title = {Skin Hunger: A Telematic Installation},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538010},
doi = {10.1145/3537972.3538010},
abstract = {Skin Hunger is a web-based interactive system for telematic installation and performance that plays on the zoom-style video-chat that has become ubiquitous during the COVID-19 pandemic. Participants in the telematic installation can reach across the webcam screens to virtually ‘touch’ one another. By touching or moving together, participants create virtual organisms and sounds that emerge and evolve from participant relation and interaction, making the intangible connection tangible and giving it life. Skin Hunger explores a new way of enabling embodied joint music making and movement over a distance.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {30},
numpages = {3},
keywords = {NIME, Embodiment, HCI, Telematic Art, Interactive Dance},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538012,
author = {Carlson, Kristin and Irannezhad, Zahra and Carney, Laina},
title = {Desquamation},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538012},
doi = {10.1145/3537972.3538012},
abstract = {Desquamation is a three-part choreographic work that challenges viewer experience of socio-cultural identity theories through an interactive, 360 degree-video lens. This work seeks to explore the impact of interactive three-hundred-and-sixty (360) degree- video through dance and socio-cultural theory. In this work, the power dynamics of the traditional performer-to-audience relationship shifts, introducing a new layer of interaction into the [virtual] performance space. Traditionally, audience members viewing dance-films are asked to be passive in their engagement with the work. In this exploration, project collaborators seek to transform the viewer's experience as an audience member from static to active. The objective of the selected performance medium serves to provide audience members agency, by being in control of their own gaze, in their experience and interpretation of the work. Audience member experiences vary upon a number of factors including (but not limited to): order of videos viewed, location of dancer on-screen, setting, choreographic and/or sonic meaning-making, and/or lived experience. Desquamation aims to exhibit the interconnectedness of socio-cultural voyeurism, dance, and race through a fresh, interactive lens of 360° video technology. The interdisciplinary collaborations explored in this work demonstrate the use of technology, choreographic and cultural movement practices, and theoretical and philosophical perspectives on movement that aim to diversify the spectrum of movement computing and technology in the future.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {31},
numpages = {3},
keywords = {360-degree Video, Audience Agency, Socio-Cultural Voyeurism, Dance, Race},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538014,
author = {Burns, Tara},
title = {Extending the Body: An Embodied Practice as Choreography in Virtual Reality Performance},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538014},
doi = {10.1145/3537972.3538014},
abstract = {This paper chronicles the exposition of the artistic practice and process for Extending the Body which utilizes an embodied dance practice with Virtual Reality (VR) as a partner in live streamed performance. Situated in phenomenology and the affective response of the body, the performer instigates a call and response between themselves and the virtual reality painting program, Tilt Brush, that can be viewed from the physical and virtual world. When viewed in the physical world, additional elements of prosthetic Othering are witnessed, argued to present as “theatrical prosthesis” and cyborg feminism. When viewed in the virtual world, the body of the performer is absent. Live performance results in both a dual presence and layered perspective by both performer and audience.Extending the Body is the result of a three-year research process investigated within proscenium, installation, multi-dancer rehearsal processes, and research presentations that delve into the autoethnographic, phenomenological, and performative drawing lineage of the practice. More investigation is needed to understand future ramifications of this practice, but one element is clear: working with VR as a partner requires the same empathetic response as human collaboration and perhaps in the future, this could result in more empathetic relationships with technology.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {32},
numpages = {6},
keywords = {embodiment in VR, virtual touch, installation, live performance with VR, improvisation as choreography, VR as a partner, virtual and physical environments},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538015,
author = {Bergner, Yoav and Payne, William and McDermott, Kathleen and Desportes, Kayla},
title = {DanceON and SoftWEAR: Education-Level Creative Coding and Programmable Wearables},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538015},
doi = {10.1145/3537972.3538015},
abstract = {Our research collaborative has been exploring movement computing educational technology experiences. That is, we have been building tools that simultaneously support both movement and computing learning objectives at entry-level. We will demo two products in development. danceON is a domain-specific language and a web app that allows users to create interactive graphics overlaid on video from pre-recorded or live (webcam) sources. softWEAR is a solderless and breadboardless ecosystem using sensors, LEDs, and the Adafruit Trinket M0. It is designed to support a workflow from ideation, prototyping, and iteration to a durable, wearable final project embedded into clothing or accessories.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {33},
numpages = {3},
keywords = {creative coding, movement computing education, wearables},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538026,
author = {Percy, Max},
title = {Haptic Technology and Motion Capture to Make Dance More Accessible for the Blind and Visually Impaired},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538026},
doi = {10.1145/3537972.3538026},
abstract = {This practice work seeks to explore how haptic and motion capture technology might be utilised to make the act of experiencing dance more accessible for a blind audience. Though visual impairment and blindness can mean differing things and speak to different experiences, this paper will utilise the term "blind audience" in reference to the group this technology will focus on. I had three guiding principles in this work:•&nbsp;Can dance be translated into a haptic experience for those who are visually impaired/blind?•&nbsp;Can this be done through designing a wearable garment that houses the haptic technology?•&nbsp;Can this experience shape or alter an individual's understanding of dance?},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {34},
numpages = {3},
keywords = {haptic technology, kinaesthetic empathy, motion capture, Wearable technology, accessibility},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538017,
author = {Kemper, Steven},
title = {Manus Tremens},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538017},
doi = {10.1145/3537972.3538017},
abstract = {Manus Tremens is a structured improvisation for accelerometer-controlled vibration motors, amplified toy harp, and live sound processing. The performer uses two vibration motors to directly actuate the strings of a toy harp. Accelerometers affixed to the performer's wrists affect the vibration intensity of each motor, which enables nuanced dynamic control of the sounds being produced. The design of this system and performance are inspired by the cimbalom, an Eastern European hammered dulcimer instrument. The Manus Tremens system also represents a collaborative approach to sound-making that combines human movements and mechatronic sound actuation.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {35},
numpages = {2},
keywords = {Collaborative Robotics, Interactive Music System, Gestural Control, Musical Robotics},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538019,
author = {Ingebritsen, Ryan and Knowlton, Christopher},
title = {And She Will Sound The Alarm: Performance and Demo of the Body Sample Player},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538019},
doi = {10.1145/3537972.3538019},
abstract = {Kinesthetic empathy, in the context of interactive design, is defined as the ability to encode and decode the input of other users of the system or to sense a shift in the system itself [7]. Achieving this between two players or between players and observers in a virtual performance system involving sound is difficult to achieve and requires that players and audience have an existing connection with the system that marries the sonic and kinesthetic sense known as “auditory kinesthesia” [11]. These sensibilities can be facilitated through the design of the system itself following the principles of usability and kinesthetic interaction, or through performer training [9]. In traditional music and dance, kinesthetic empathy exists both between players and between players and audience. It is much more difficult to achieve this in the context of interactive performance as the audience does not have the same culturally specific empathetic response when observing digitally mediated performance as they do when viewing forms of music and dance they are already familiar with. The Body Sample Player is a digital musical instrument and performance platform that uses real-time data gathered by body tracking cameras to control the volumes of looping sound samples, turning the body itself into a sound controller. The system has evolved over the years, facilitating several different musical works ranging from solo ”DJ” controller style musical performances to large scale interactive music and dance works for multiple performers. It has also been used to create interactive installations that are accessible to the audiences of these large-scale works. It is our contention that by giving audience members first-hand experience engaging with this instrument before witnessing a performance, they will more easily be able to recognize expressivity in the performance, and in doing so, will experience a higher level of kinesthetic empathy. Therefore, we propose a performance of And She Will Sound the Alarm, a piece for a single performer created for the Body Sample Player system, to be preceded by a demonstration in which conference attendees can interact with the system itself.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {36},
numpages = {4},
keywords = {kinesthetic empathy, sonification, entrainment, digital musical instruments, contemporary dance, auditory kinesthesia},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538020,
author = {Derry, Lins and Kruguer, Jordan and Mueller, Maximilian and Schnapp, Jeffrey},
title = {Designing a Choreographic Interface During COVID-19},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538020},
doi = {10.1145/3537972.3538020},
abstract = {In 2019, metaLAB (at) Harvard began work on Curatorial A(i)gents, a digital exhibition that was slated to premiere at the Harvard Art Museums’ Lightbox Gallery in 2020. Half of the projects would be interactive, using mouse and keyboard conventions. With the advent of Covid-19 and the postponement of the show, the authors set out to develop an interface solution that would enable visitors to interact with the works without having to touch any public devices like a tablet. Toward this end, we prototyped a “choreographic interface” that uses machine vision and machine learning to interpret a full-torso gestural vocabulary, which is then translated into interactions. To make the choreographic interface, we relied on open-source solutions, which have all come with equal limitations and opportunities. In 2022, Curatorial A(i)gents was presented in the Lightbox Gallery, where we had the opportunity to test and demonstrate the interface. This paper discusses our design journey in making a choreographic interface using open-source technologies during Covid-19.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {37},
numpages = {7},
keywords = {Choreography, Choreographic interfaces, Embodied interaction, Covid-19, Dance, User interface, Machine learning, Design},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538021,
author = {Jarzombek, Elias and Wang, Yiran Marcel},
title = {Pendular},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538021},
doi = {10.1145/3537972.3538021},
abstract = {Pendular is an interactive musical system that explores the physical motion of hanging objects as an interface for creating music. Performers use wind and light to control the environment to which these objects react, and as they swing and spin through the air, their movements are interpreted as sonic transformations ranging from calm and predictable to chaotic and random.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {38},
numpages = {3},
keywords = {musical interface, NIME, musical performance, music, sound, interactive art, physics},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538022,
author = {Meshi, Avital and Chiaravalloti, Treyden},
title = {InVisible - Movement on the Edge of Two Realities},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538022},
doi = {10.1145/3537972.3538022},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {39},
numpages = {2},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538023,
author = {Laviers, Amy and Maguire, Cat},
title = {The BESST System: Explicating a New Component of Time in Laban/Bartenieff Movement Studies Through Work With Robots},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538023},
doi = {10.1145/3537972.3538023},
abstract = {In order to develop interactive technology that interfaces more and more with humans and their full embodiment, robotics engineers need a means for reasoning about natural and artificial movement that is rooted in human experience. Movement studies – a broad field covering somatics and choreography – can act as a bridge to allow engineers to create technology that interacts with human movement in order to serve societal needs. An interesting, perhaps unanticipated result of this process is that applying movement studies to robotics requires a sharpening of the methodology. Thus, the branch of movement studies initiated by Laban and Bartenieff – sometimes described as Laban/Bartenieff Movement Studies (LBMS) – provides an important tool set for understanding movement patterns in context. This work is organized through four established components: Body, Effort, Shape, and Space (or the BESS System). The observation and movement-based workshop described in this extended abstract will share ideas about temporal patterns in movement, explicated as a new component, Time, establishing the BESST System. This component grows from working with machines, which must deal in user-specified and designed quantitative units of time, as a way to describe motion that is not quite as fluent as human motion. Some machines do not portray clear ideas about intent and relationship from their movement, but it is typically possible to measure the amount of time an action took and frequently phrasing is observed through stops and starts of different machine parts. Thus, while a particular example of artificial movement may not rise to the level of creating a clear dynamic quality, it does use elements collected in this component of Time, e.g., sequencing, duration, tempo, and phrasing. This workshop will offer an experiential inroad to these elements and their use inside of contemporary approaches to robotics.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {40},
numpages = {3},
keywords = {robotics, HRI, movement studies, embodiment, style, Laban Movement Analysis, expressivity, Laban/Bartenieff Movement Studies},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3537972.3538025,
author = {Carmichael, Renee},
title = {Blockchain Feels: A Workshop in Choreographing Complexity},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538025},
doi = {10.1145/3537972.3538025},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {41},
numpages = {4},
keywords = {Blockchain, Language, Choreography, Body, Movement},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

